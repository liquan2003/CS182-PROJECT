#!/usr/bin/python3
# -*- encoding: utf-8 -*-

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import xgboost as xgb
import time

DAYS_FOR_TRAIN = 10  # ç”¨è¿‡å»10å¤©é¢„æµ‹ç¬¬11å¤©


def create_dataset(data, days_for_train=10):
    dataset_x, dataset_y = [], []
    for i in range(len(data) - days_for_train):
        _x = data[i:(i + days_for_train)]
        dataset_x.append(_x)
        dataset_y.append(data[i + days_for_train])
    return np.array(dataset_x), np.array(dataset_y)


if __name__ == '__main__':
    t0 = time.time()

    print("ğŸ“¥ Step 1: è¯»å–æ•°æ®...")
    data_close = pd.read_csv('MSFT_price.csv')
    data_close = data_close.astype('float32').values.flatten()  # ä¿è¯æ˜¯ä¸€ç»´å‘é‡
    print(f"âœ… æ•°æ®é•¿åº¦: {len(data_close)}")

    print("ğŸ“Š Step 2: ç»˜åˆ¶åŸå§‹ä»·æ ¼å›¾...")
    plt.plot(data_close)
    plt.title("Raw MSFT Prices")
    plt.savefig('xgb_data.png')
    plt.close()

    print("ğŸ§® Step 3: æ ‡å‡†åŒ–æ•°æ®...")
    max_value = np.max(data_close)
    min_value = np.min(data_close)
    data_close_scaled = (data_close - min_value) / (max_value - min_value + 1e-8)

    print("ğŸ§± Step 4: æ„é€ æ•°æ®é›†...")
    dataset_x, dataset_y = create_dataset(data_close_scaled, DAYS_FOR_TRAIN)
    dataset_x = dataset_x.reshape(dataset_x.shape[0], -1)  # âœ… è½¬ä¸ºäºŒç»´
    print(f"âœ… ç‰¹å¾ç»´åº¦: {dataset_x.shape}, æ ‡ç­¾ç»´åº¦: {dataset_y.shape}")

    train_size = int(len(dataset_x) * 0.7)
    train_x = dataset_x[:train_size]
    train_y = dataset_y[:train_size]
    test_x = dataset_x[train_size:]
    test_y = dataset_y[train_size:]
    print(f"âœ… è®­ç»ƒæ ·æœ¬æ•°: {len(train_x)}, æµ‹è¯•æ ·æœ¬æ•°: {len(test_x)}")

    print("ğŸ“¦ Step 5: è½¬æ¢ä¸º DMatrix æ ¼å¼...")
    dtrain = xgb.DMatrix(train_x, label=train_y)
    dtest = xgb.DMatrix(test_x, label=test_y)

    print("âš™ï¸ Step 6: è®¾ç½® XGBoost å‚æ•°å¹¶è®­ç»ƒæ¨¡å‹...")
    params = {
        'booster': 'gbtree',
        'objective': 'reg:squarederror',
        'max_depth': 5,
        'eta': 0.05,
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'seed': 42
    }

    num_round = 200
    model = xgb.train(params, dtrain, num_round)
    print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆã€‚")

    print("ğŸ” Step 7: æ¨¡å‹é¢„æµ‹...")
    pred_train = model.predict(dtrain)
    pred_test = model.predict(dtest)
    pred_all = np.concatenate([pred_train, pred_test])
    pred_all = np.concatenate((np.zeros(DAYS_FOR_TRAIN), pred_all))

    assert len(pred_all) == len(data_close_scaled)

    print("ğŸ“ˆ Step 8: ç»˜å›¾å±•ç¤ºç»“æœ...")
    plt.figure(figsize=(10, 5))
    plt.plot(pred_all, 'r', label='Prediction')
    plt.plot(data_close_scaled, 'b', label='Real')
    plt.axvline(x=train_size + DAYS_FOR_TRAIN, color='g', linestyle='--', label='Train/Test Split')
    plt.legend()
    plt.title("XGBoost (No Sentiment) - Prediction vs Real")
    plt.xlabel("Days")
    plt.ylabel("Normalized Close Price")
    plt.tight_layout()
    plt.savefig('MSFT_result_xgb.png')
    plt.close()

    t1 = time.time()
    print("â±ï¸ Step 9: Done! Training time: %.2f mins" % ((t1 - t0) / 60))
